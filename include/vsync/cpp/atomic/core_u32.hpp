/*
 * Copyright (C) Huawei Technologies Co., Ltd. 2025. All rights reserved.
 * SPDX-License-Identifier: MIT
 */
#ifndef VATOMIC_CORE_U32_HPP
#define VATOMIC_CORE_U32_HPP
/*!!!Warning: File generated by tmplr; DO NOT EDIT.!!!*/
extern "C" {
#include <vsync/atomic.h>
}

namespace vsync
{
    template <> struct atomic<vuint32_t> {
        atomic(const atomic &)                     = delete;
        atomic &operator=(const atomic &)          = delete;
        atomic &operator=(const atomic &) volatile = delete;

        atomic()
        {
            vatomic32_init(&_v, 0U);
        }
        atomic(vuint32_t v)
        {
            vatomic32_init(&_v, v);
        }

        vuint32_t load(memory_order order = memory_order_seq_cst) const noexcept
        {
            switch (order) {
                case memory_order_consume:
                case memory_order_acquire:
                    return (vatomic32_read_acq(&_v));
                case memory_order_relaxed:
                    return (vatomic32_read_rlx(&_v));
                case memory_order_release:
                case memory_order_acq_rel:
                case memory_order_seq_cst:
                default:
                    return (vatomic32_read(&_v));
            }
        }
        void store(vuint32_t v,
                   memory_order order = memory_order_seq_cst) noexcept
        {
            switch (order) {
                case memory_order_release:
                    vatomic32_write_rel(&_v, v);
                    break;
                case memory_order_relaxed:
                    vatomic32_write_rlx(&_v, v);
                    break;
                case memory_order_acquire:
                case memory_order_acq_rel:
                case memory_order_consume:
                case memory_order_seq_cst:
                default:
                    return vatomic32_write(&_v, v);
            }
        }

        vuint32_t operator=(vuint32_t v) noexcept
        {
            store(v);
            return v;
        }

        operator vuint32_t() const noexcept
        {
            return load();
        }

        vuint32_t exchange(vuint32_t v,
                           memory_order order = memory_order_seq_cst) noexcept
        {
            switch (order) {
                case memory_order_release:
                    return (vatomic32_xchg_rel(&_v, v));
                case memory_order_relaxed:
                    return (vatomic32_xchg_rlx(&_v, v));
                case memory_order_consume:
                case memory_order_acquire:
                    return (vatomic32_xchg_acq(&_v, v));
                case memory_order_acq_rel:
                case memory_order_seq_cst:
                default:
                    return (vatomic32_xchg(&_v, v));
            }
        }

        bool compare_exchange_strong(
            vuint32_t &expected, vuint32_t desired,
            memory_order order   = memory_order_seq_cst,
            memory_order failure = memory_order_seq_cst) noexcept
        {
            vuint32_t old = 0;
            switch (order) {
                case memory_order_release:
                    old = (vatomic32_cmpxchg_rel(&_v, expected, desired));
                    break;
                case memory_order_relaxed:
                    old = (vatomic32_cmpxchg_rlx(&_v, expected, desired));
                    break;
                case memory_order_consume:
                case memory_order_acquire:
                    old = (vatomic32_cmpxchg_acq(&_v, expected, desired));
                    break;
                case memory_order_acq_rel:
                case memory_order_seq_cst:
                default:
                    old = (vatomic32_cmpxchg(&_v, expected, desired));
                    break;
            }
            if (old == expected) {
                return true;
            } else {
                expected = old;
                return false;
            }
        }
        bool compare_exchange_weak(
            vuint32_t &expected, vuint32_t desired,
            memory_order order   = memory_order_seq_cst,
            memory_order failure = memory_order_seq_cst) noexcept
        {
            return compare_exchange_strong(expected, desired, order, failure);
        }

        vuint32_t fetch_add(vuint32_t v,
                            memory_order order = memory_order_seq_cst) noexcept
        {
            switch (order) {
                case memory_order_release:
                    return (vatomic32_get_add_rel(&_v, v));
                case memory_order_relaxed:
                    return (vatomic32_get_add_rlx(&_v, v));
                case memory_order_consume:
                case memory_order_acquire:
                    return (vatomic32_get_add_acq(&_v, v));
                case memory_order_acq_rel:
                case memory_order_seq_cst:
                default:
                    return (vatomic32_get_add(&_v, v));
            }
        }
        vuint32_t operator+=(vuint32_t v) noexcept
        {
            return fetch_add(v);
        }
        // v++
        vuint32_t operator++(int) noexcept
        {
            return vatomic32_get_inc(&_v);
        }
        // ++v
        vuint32_t operator++() noexcept
        {
            return vatomic32_inc_get(&_v);
        }

        vuint32_t fetch_sub(vuint32_t v,
                            memory_order order = memory_order_seq_cst) noexcept
        {
            switch (order) {
                case memory_order_release:
                    return vatomic32_get_sub_rel(&_v, v);
                case memory_order_relaxed:
                    return vatomic32_get_sub_rlx(&_v, v);
                case memory_order_consume:
                case memory_order_acquire:
                    return vatomic32_get_sub_acq(&_v, v);
                case memory_order_acq_rel:
                case memory_order_seq_cst:
                default:
                    return vatomic32_get_sub(&_v, v);
            }
        }
        vuint32_t operator-=(vuint32_t v) noexcept
        {
            return fetch_sub(v);
        }
        // v--
        vuint32_t operator--(int) noexcept
        {
            return vatomic32_get_dec(&_v);
        }
        // --v
        vuint32_t operator--() noexcept
        {
            return vatomic32_dec_get(&_v);
        }

        vuint32_t fetch_and(vuint32_t v,
                            memory_order order = memory_order_seq_cst) noexcept
        {
            switch (order) {
                case memory_order_release:
                    return vatomic32_get_and_rel(&_v, v);
                case memory_order_relaxed:
                    return vatomic32_get_and_rlx(&_v, v);
                case memory_order_consume:
                case memory_order_acquire:
                    return vatomic32_get_and_acq(&_v, v);
                case memory_order_acq_rel:
                case memory_order_seq_cst:
                default:
                    return vatomic32_get_and(&_v, v);
            }
        }

        vuint32_t operator&=(vuint32_t v) noexcept
        {
            return fetch_and(v);
        }

        vuint32_t fetch_or(vuint32_t v,
                           memory_order order = memory_order_seq_cst) noexcept
        {
            switch (order) {
                case memory_order_release:
                    return vatomic32_get_or_rel(&_v, v);
                case memory_order_relaxed:
                    return vatomic32_get_or_rlx(&_v, v);
                case memory_order_consume:
                case memory_order_acquire:
                    return vatomic32_get_or_acq(&_v, v);
                case memory_order_acq_rel:
                case memory_order_seq_cst:
                default:
                    return vatomic32_get_or(&_v, v);
            }
        }

        vuint32_t operator|=(vuint32_t v) noexcept
        {
            return fetch_or(v);
        }

        vuint32_t fetch_xor(vuint32_t v,
                            memory_order order = memory_order_seq_cst) noexcept
        {
            switch (order) {
                case memory_order_release:
                    return vatomic32_get_xor_rel(&_v, v);
                case memory_order_relaxed:
                    return vatomic32_get_xor_rlx(&_v, v);
                case memory_order_consume:
                case memory_order_acquire:
                    return vatomic32_get_xor_acq(&_v, v);
                case memory_order_acq_rel:
                case memory_order_seq_cst:
                default:
                    return vatomic32_get_xor(&_v, v);
            }
        }

        vuint32_t operator^=(vuint32_t v) noexcept
        {
            return fetch_xor(v);
        }
        bool is_lock_free() const noexcept
        {
            return true;
        }

      private:
        vatomic32_t _v;
    };
}; // namespace vsync

#endif
